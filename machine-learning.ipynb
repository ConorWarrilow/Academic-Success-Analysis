{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "# Tools\n",
    "from copy import copy # Shallow copy\n",
    "from itertools import product\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from IPython.display import display # Allows functions to simultaneously return values and show tables\n",
    "\n",
    "# Styling\n",
    "from colorama import Fore\n",
    "from colorama import Style\n",
    "from matplotlib.colors import Colormap\n",
    "\n",
    "\n",
    "# Assessing Feature Importance\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from sklearn.preprocessing import Binarizer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# t-SNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# Dendogram\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "# Kde Plots\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "\n",
    "\n",
    "# Probability plots\n",
    "from scipy.stats import probplot\n",
    "\n",
    "# The Tree Trio\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Good ol utils\n",
    "from utils import *\n",
    "\n",
    "# Sequential Feature Selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler, CmaEsSampler\n",
    "import optuna.visualization as vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(r\"C:\\Users\\Nebula PC\\Desktop\\Projects\\Academic-Success-Prediction\\data\\train.csv\", index_col=\"id\").rename(columns=str.strip)\n",
    "test = pd.read_csv(r\"C:\\Users\\Nebula PC\\Desktop\\Projects\\Academic-Success-Prediction\\data\\test.csv\", index_col=\"id\").rename(columns=str.strip)\n",
    "\n",
    "target = \"Target\"\n",
    "\n",
    "value_mapping = {\n",
    "    'Enrolled': 2,\n",
    "    'Dropout': 0,\n",
    "    'Graduate': 1\n",
    "}\n",
    "\n",
    "# Replace the values in the \"Target\" column\n",
    "train['Target'] = train['Target'].replace(value_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOTTOM_15_FEATURES = [\n",
    "    'International',\n",
    "    'Educational special needs',\n",
    "    'Nacionality',\n",
    "    'Marital status',\n",
    "    'Inflation rate',\n",
    "    'Curricular units 1st sem (without evaluations)',\n",
    "    'Father\\'s qualification',\n",
    "    'Mother\\'s qualification',\n",
    "    'Displaced',\n",
    "    'Curricular units 2nd sem (credited)',\n",
    "    'Curricular units 1st sem (credited)',\n",
    "    'Application order',\n",
    "    'Previous qualification',\n",
    "    'Daytime/evening attendance',\n",
    "    'Curricular units 2nd sem (without evaluations)'\n",
    "]\n",
    "\n",
    "train = train.drop(columns = BOTTOM_15_FEATURES)\n",
    "test = test.drop(columns = BOTTOM_15_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_12_FEATURES = [\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)',\n",
    "    'Tuition fees up to date',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 2nd sem (evaluations)',\n",
    "    'Curricular units 2nd sem (enrolled)',\n",
    "    'Scholarship holder',\n",
    "    'Curricular units 1st sem (evaluations)',\n",
    "    'Course',\n",
    "    'Curricular units 1st sem (enrolled)'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "ONE_HOT_COLUMNS_SELECTED = [\n",
    "                   'Application mode', \n",
    "                   'Course', \n",
    "                   'Mother\\'s occupation', \n",
    "                   'Father\\'s occupation']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BINARY_INDICATOR_FEATURES_SELECTED = [\n",
    "                             'Curricular units 2nd sem (grade)',\n",
    "                             'Curricular units 2nd sem (approved)',\n",
    "                             'Curricular units 2nd sem (evaluations)',\n",
    "                             'Curricular units 1st sem (grade)',\n",
    "                             'Curricular units 1st sem (approved)',\n",
    "                             'Curricular units 1st sem (evaluations)',\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "BINARY_COLUMNS = []\n",
    "for column in train.columns:\n",
    "    if len(train[column].value_counts()) == 2:\n",
    "        BINARY_COLUMNS.append(column)\n",
    "\n",
    "binary_data = train[BINARY_COLUMNS+ ['Target']]\n",
    "\n",
    "\n",
    "\n",
    "NUMERIC_COLUMNS = train.drop(columns = ONE_HOT_COLUMNS_SELECTED + BINARY_COLUMNS + ['Target']).columns.to_list()\n",
    "numeric_data = train[NUMERIC_COLUMNS + ['Target']]\n",
    "\n",
    "\n",
    "\n",
    "FLOAT_COLUMNS = train[NUMERIC_COLUMNS].select_dtypes('float').columns.to_list()\n",
    "float_data = train[FLOAT_COLUMNS + ['Target']]\n",
    "\n",
    "\n",
    "\n",
    "INTEGER_COLUMNS = train[NUMERIC_COLUMNS].select_dtypes('int').columns.to_list()\n",
    "integer_data = train[INTEGER_COLUMNS + ['Target']]\n",
    "\n",
    "\n",
    "positive_features = list(train[NUMERIC_COLUMNS].describe().T.query(\"min > 0\").index)\n",
    "zero_features = list(train[NUMERIC_COLUMNS].describe().T.query(\"min == 0\").index)\n",
    "negative_features = list(train[NUMERIC_COLUMNS].describe().T.query(\"min < 0\").index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df['sum_evaluations'] = df['Curricular units 2nd sem (evaluations)'] + df['Curricular units 1st sem (evaluations)']\n",
    "    df['min_evaluations'] = df[['Curricular units 2nd sem (evaluations)', 'Curricular units 1st sem (evaluations)']].min(axis = 1)\n",
    "    df['max_evaluations'] = df[['Curricular units 2nd sem (evaluations)', 'Curricular units 1st sem (evaluations)']].max(axis = 1)\n",
    "    df['difference_evaluations'] = abs(df['Curricular units 2nd sem (evaluations)'] - df['Curricular units 1st sem (evaluations)'])\n",
    "    df['change_evaluations'] = df['Curricular units 2nd sem (evaluations)'] - df['Curricular units 1st sem (evaluations)']\n",
    "\n",
    "    df['sum_enrolled'] = df['Curricular units 2nd sem (enrolled)'] + df['Curricular units 1st sem (enrolled)']\n",
    "    df['min_enrolled'] = df[['Curricular units 2nd sem (enrolled)', 'Curricular units 1st sem (enrolled)']].min(axis = 1)\n",
    "    df['max_enrolled'] = df[['Curricular units 2nd sem (enrolled)', 'Curricular units 1st sem (enrolled)']].max(axis = 1)\n",
    "    df['difference_enrolled'] = abs(df['Curricular units 2nd sem (enrolled)'] - df['Curricular units 1st sem (enrolled)'])\n",
    "    df['change_enrolled'] = df['Curricular units 2nd sem (enrolled)'] - df['Curricular units 1st sem (enrolled)']\n",
    "\n",
    "\n",
    "    #IMPORTANT\n",
    "    df['sum_grade'] = df['Curricular units 2nd sem (grade)'] + df['Curricular units 1st sem (grade)']\n",
    "    df['min_grade'] = df[['Curricular units 2nd sem (grade)', 'Curricular units 1st sem (grade)']].min(axis = 1)\n",
    "    df['max_grade'] = df[['Curricular units 2nd sem (grade)', 'Curricular units 1st sem (grade)']].max(axis = 1)\n",
    "    df['difference_grade'] = abs(df['Curricular units 2nd sem (grade)'] - df['Curricular units 1st sem (grade)'])\n",
    "    df['change_grade'] = df['Curricular units 2nd sem (grade)'] - df['Curricular units 1st sem (grade)']\n",
    "\n",
    "\n",
    "\n",
    "    #IMPORTANT\n",
    "    df['sum_approved'] = df['Curricular units 2nd sem (approved)'] + df['Curricular units 1st sem (approved)']\n",
    "    df['min_approved'] = df[['Curricular units 2nd sem (approved)', 'Curricular units 1st sem (approved)']].min(axis = 1)\n",
    "    df['max_approved'] = df[['Curricular units 2nd sem (approved)', 'Curricular units 1st sem (approved)']].max(axis = 1)\n",
    "    df['difference_approved'] = abs(df['Curricular units 2nd sem (approved)'] - df['Curricular units 1st sem (approved)'])\n",
    "    df['change_approved'] = df['Curricular units 2nd sem (approved)'] - df['Curricular units 1st sem (approved)']\n",
    "\n",
    "\n",
    "    #Interactions using important features\n",
    "    df['approved_minus_grade_s1'] = df['Curricular units 1st sem (approved)'] - df['Curricular units 1st sem (grade)']\n",
    "    df['grade_minus_approved_s1'] = df['Curricular units 1st sem (grade)'] - df['Curricular units 1st sem (approved)']\n",
    "    df['approved_minus_grade_s2'] = df['Curricular units 2nd sem (approved)'] - df['Curricular units 2nd sem (grade)']\n",
    "    df['grade_minus_approved_s2'] = df['Curricular units 2nd sem (grade)'] - df['Curricular units 2nd sem (approved)']\n",
    "\n",
    "    df['approved_add_grade_s1'] = df['Curricular units 1st sem (approved)'] + df['Curricular units 1st sem (grade)']\n",
    "    df['approved_add_grade_s2'] = df['Curricular units 2nd sem (approved)'] + df['Curricular units 2nd sem (grade)']\n",
    "\n",
    "    df['approved_add_grade_s1_s2'] = df['approved_add_grade_s1'] + df['approved_add_grade_s2']\n",
    "\n",
    "    #other interactions\n",
    "    df['curricular_units_sum_s1'] = df['Curricular units 1st sem (evaluations)'] + df['Curricular units 1st sem (enrolled)'] + df['Curricular units 1st sem (grade)'] + df['Curricular units 1st sem (approved)']\n",
    "    df['curricular_units_sum_s2'] = df['Curricular units 2nd sem (evaluations)'] + df['Curricular units 2nd sem (enrolled)'] + df['Curricular units 2nd sem (grade)'] + df['Curricular units 2nd sem (approved)']\n",
    "    df['curricular_units_sum_s1_s2'] = df['curricular_units_sum_s1'] + df['curricular_units_sum_s2']\n",
    "    df['curricular_units_difference_s1_s2'] = abs(df['curricular_units_sum_s2'] - df['curricular_units_sum_s1'])\n",
    "    df['curricular_units_change_s1_s2'] = df['curricular_units_sum_s2'] - df['curricular_units_sum_s1']\n",
    "\n",
    "    \n",
    "    df['fees_plus_scholarship'] = df['Scholarship holder'] + 2*df['Tuition fees up to date']  # Use 2* so we can differentiate between scholarship and fees\n",
    "\n",
    "    for col in BINARY_INDICATOR_FEATURES_SELECTED:\n",
    "        df[f'{col} (binary)'] = (df[col] > 0).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = feature_engineering(train)\n",
    "test = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLUMNS_ENGINEERED = train.drop(columns = ONE_HOT_COLUMNS_SELECTED + BINARY_COLUMNS + ['Target']).columns.to_list()\n",
    "numeric_data = train[NUMERIC_COLUMNS_ENGINEERED + ['Target']]\n",
    "\n",
    "positive_features = list(train[NUMERIC_COLUMNS_ENGINEERED].describe().T.query(\"min > 0\").index)\n",
    "zero_features = list(train[NUMERIC_COLUMNS_ENGINEERED].describe().T.query(\"min == 0\").index)\n",
    "negative_features = list(train[NUMERIC_COLUMNS_ENGINEERED].describe().T.query(\"min < 0\").index)\n",
    "\n",
    "r2_scores = defaultdict(tuple)\n",
    "\n",
    "for feature in NUMERIC_COLUMNS_ENGINEERED:\n",
    "    orig = numeric_data[feature].dropna()\n",
    "    if feature in positive_features:\n",
    "        _, (*_, R_orig) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_log) = probplot(np.log(orig), rvalue=True)\n",
    "        _, (*_, R_log1p) = probplot(np.log1p(orig), rvalue=True)\n",
    "        #_, (*_, R_exp) = probplot(np.exp(orig), rvalue=True)\n",
    "        _, (*_, R_sqrt) = probplot(np.sqrt(orig), rvalue=True)\n",
    "        _, (*_, R_square) = probplot(np.square(orig), rvalue=True)\n",
    "        _, (*_, R_reci) = probplot(np.reciprocal(orig), rvalue=True)\n",
    "        _, (*_, R_boxcox) = probplot(stats.boxcox(orig)[0], rvalue=True)\n",
    "        _, (*_, R_yeojohn) = probplot(stats.yeojohnson(orig)[0], rvalue=True)\n",
    "    elif feature in zero_features:\n",
    "        _, (*_, R_orig) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_log) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_log1p) = probplot(np.log1p(orig), rvalue=True)\n",
    "        #_, (*_, R_exp) = probplot(np.exp(orig), rvalue=True)\n",
    "        _, (*_, R_sqrt) = probplot(np.sqrt(orig), rvalue=True)\n",
    "        _, (*_, R_square) = probplot(np.square(orig), rvalue=True)\n",
    "        _, (*_, R_reci) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_boxcox) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_yeojohn) = probplot(stats.yeojohnson(orig)[0], rvalue=True)\n",
    "\n",
    "    elif feature in negative_features:\n",
    "        _, (*_, R_orig) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_log) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_log1p) = probplot(orig, rvalue=True)\n",
    "        #_, (*_, R_exp) = probplot(np.exp(orig), rvalue=True)\n",
    "        _, (*_, R_sqrt) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_square) = probplot(np.square(orig), rvalue=True)\n",
    "        _, (*_, R_reci) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_boxcox) = probplot(orig, rvalue=True)\n",
    "        _, (*_, R_yeojohn) = probplot(stats.yeojohnson(orig)[0], rvalue=True)\n",
    "\n",
    "    r2_scores[feature] = (\n",
    "        R_orig * R_orig,\n",
    "        R_log * R_log,\n",
    "        R_log1p * R_log1p,\n",
    "        #R_exp * R_exp,\n",
    "        R_sqrt * R_sqrt,\n",
    "        R_square * R_square,\n",
    "        R_reci * R_reci,\n",
    "        R_boxcox * R_boxcox,\n",
    "        R_yeojohn * R_yeojohn\n",
    "    )\n",
    "\n",
    "r2_scores = pd.DataFrame(\n",
    "    r2_scores, index=(\"Original\", \"Log\", \"Log1p\", \"Sqrt\", \"Square\", \"Reciprocal\", \"BoxCox\", \"YeoJohnson\")\n",
    ").T\n",
    "\n",
    "r2_scores[\"HighestScore\"] = r2_scores[[\"Original\", \"Log\", \"Log1p\", \"Sqrt\", \"Square\", \"Reciprocal\", \"BoxCox\", \"YeoJohnson\"]].max(axis = 1)\n",
    "r2_scores[\"Winner\"] = r2_scores.idxmax(axis=1)\n",
    "\n",
    "\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return [f'background-color: {TEAL}' if v else '' for v in is_max]\n",
    "\n",
    "r2_scores['Improvement'] = r2_scores['HighestScore'] - r2_scores['Original']\n",
    "r2_scores.style.set_table_styles(DF_STYLE).apply(highlight_max, subset= [\"Original\", \"Log\", \"Log1p\", \"Sqrt\", \"Square\", \"Reciprocal\", \"BoxCox\", \"YeoJohnson\"], axis=1).background_gradient(\n",
    "                                                                                                                            cmap = DF_CMAP2, subset = 'Improvement').format(precision = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_transform_cols = r2_scores.query(\"Improvement < 0.01\").index\n",
    "log_transform_cols = r2_scores.query(\"Winner == 'Log' & Improvement >= 0.01\").index\n",
    "log1p_transform_cols = r2_scores.query(\"Winner == 'Log1p' & Improvement >= 0.01\").index\n",
    "sqrt_transform_cols = r2_scores.query(\"Winner == 'Sqrt' & Improvement >= 0.01\").index\n",
    "square_transform_cols = r2_scores.query(\"Winner == 'Square' & Improvement >= 0.01\").index\n",
    "reciprocal_transform_cols = r2_scores.query(\"Winner == 'Reciprocal' & Improvement >= 0.01\").index\n",
    "boxcox_transform_cols = r2_scores.query(\"Winner == 'BoxCox' & Improvement >= 0.01\").index\n",
    "yeojohnson_transform_cols = r2_scores.query(\"Winner == 'YeoJohnson' & Improvement >= 0.01\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformers = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (\n",
    "            StandardScaler(),\n",
    "            no_transform_cols.to_list(),\n",
    "        ),\n",
    "        (\n",
    "            make_pipeline(\n",
    "                FunctionTransformer(func=np.log, feature_names_out=\"one-to-one\"),\n",
    "                StandardScaler(),\n",
    "            ),\n",
    "            log_transform_cols.to_list(),\n",
    "        ),\n",
    "        (\n",
    "            make_pipeline(\n",
    "                FunctionTransformer(func=np.log1p, feature_names_out=\"one-to-one\"),\n",
    "                StandardScaler(),\n",
    "            ),\n",
    "            log1p_transform_cols.to_list(),\n",
    "        ),\n",
    "        (\n",
    "            make_pipeline(\n",
    "                FunctionTransformer(func=np.sqrt, feature_names_out=\"one-to-one\"),\n",
    "                StandardScaler(),\n",
    "            ),\n",
    "            sqrt_transform_cols.to_list(),\n",
    "        ),\n",
    "        (\n",
    "            make_pipeline(\n",
    "                FunctionTransformer(func=np.square, feature_names_out=\"one-to-one\"),\n",
    "                StandardScaler(),\n",
    "            ),\n",
    "            square_transform_cols.to_list(),\n",
    "        ),\n",
    "        (\n",
    "            PowerTransformer(method=\"box-cox\", standardize=True),\n",
    "            boxcox_transform_cols.to_list(),\n",
    "        ),\n",
    "        (\n",
    "            PowerTransformer(method=\"yeo-johnson\", standardize=True),\n",
    "            yeojohnson_transform_cols.to_list(),\n",
    "        ),\n",
    "        \n",
    "        remainder=\"passthrough\",\n",
    "        verbose_feature_names_out=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(target, axis=1)\n",
    "y = train[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.832...\n",
    "xgb_minmax_params = {'learning_rate': 0.04808572634755236, 'gamma': 9.560327584505747e-05, 'max_depth': 6, 'min_child_weight': 0.11743307971084631, 'subsample': 0.7204079538816981, 'colsample_bytree': 0.4523296155877883, 'lambda': 0.5492675245323843, 'alpha': 8.819969375852153e-08, 'n_estimators': 435}\n",
    "\n",
    "#0.831098\n",
    "xgb2_minmax_params = {'learning_rate': 0.15267345813242902, 'gamma': 0.0002202210602923774, 'max_depth': 3, 'min_child_weight': 0.0032093132626038303, 'subsample': 0.7789317921899368, 'colsample_bytree': 0.4944876602452024, 'lambda': 4.088956032379317e-05, 'alpha': 1.264701759175353e-07, 'n_estimators': 369}\n",
    "#0.831647\n",
    "xgb3_minmax_params = {'learning_rate': 0.020498556354593255, 'gamma': 4.0117054063305194e-06, 'max_depth': 9, 'min_child_weight': 9.097349005570713, 'subsample': 0.814672362304106, 'colsample_bytree': 0.5074813732573049, 'lambda': 8.132240585431906e-06, 'alpha': 3.493464282610354e-07, 'n_estimators': 479}\n",
    "\n",
    "lgbm_minmax_params = {'learning_rate': 0.12548315204042748, 'gamma': 4.578734755794731e-06, 'max_depth': 11, 'min_child_weight': 0.006074836332063258, 'subsample': 0.542429661988086, 'colsample_bytree': 0.8889283657514138, 'lambda': 0.0774449841007967, 'alpha': 0.006148549497097544, 'scale_pos_weight': 6.7529237049752915, 'n_estimators': 482, 'boosting_type': 'dart'}\n",
    "#0.83190\n",
    "lgbm2_minmax_params = {'learning_rate': 0.07311929223191145, 'gamma': 0.0001465824143858157, 'max_depth': 5, 'min_child_weight': 1.1101772255986377, 'subsample': 0.7717416516622793, 'colsample_bytree': 0.5521824473706827, 'lambda': 1.204402096691422e-08, 'alpha': 6.446090081481055e-07, 'scale_pos_weight': 3.2424584929164473, 'n_estimators': 494, 'boosting_type': 'gbdt'}\n",
    "#0.83172\n",
    "lgbm3_minmax_params = {'learning_rate': 0.05305666675975752, 'gamma': 3.737093538969974e-05, 'max_depth': 10, 'min_child_weight': 0.015065986984559972, 'subsample': 0.4107383480784791, 'colsample_bytree': 0.4754708058469771, 'lambda': 1.1778913998726071e-09, 'alpha': 0.00046716162072521424, 'scale_pos_weight': 7.601656443319444, 'n_estimators': 798, 'boosting_type': 'gbdt'}\n",
    "\n",
    "cat_minmax_params = {'learning_rate': 0.0822257813369298, 'depth': 6, 'l2_leaf_reg': 1.5125953816160678, 'iterations': 378, 'random_strength': 0.08419072361237345, 'grow_policy': 'Lossguide', 'boosting_type': 'Plain', 'bootstrap_type': 'Bernoulli'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_score(model, data, folds = 5, target='Target'): #include_original=True):\n",
    "    X = data.drop(columns = target)\n",
    "    y = data[target]\n",
    "    \n",
    "    skfold = StratifiedKFold(n_splits= folds, shuffle=True, random_state= 42) \n",
    "\n",
    "    # Initiate prediction arrays and score lists\n",
    "    val_predictions = np.zeros((len(X), 3))  # Adjust for class probabilities\n",
    "    train_scores, val_scores = [], []\n",
    "    \n",
    "    # Training model and evaluating metrics\n",
    "    for fold, (train_idx, val_idx) in enumerate(skfold.split(X, y)):\n",
    "        # Define train set\n",
    "        X_train, y_train = X.iloc[train_idx].reset_index(drop=True), y.iloc[train_idx].reset_index(drop=True)\n",
    "        \n",
    "        # Define validation set\n",
    "        X_val, y_val = X.iloc[val_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        \n",
    "        X_train = column_transformers.fit_transform(X_train)\n",
    "        X_val = column_transformers.transform(X_val)\n",
    "\n",
    "\n",
    "        #if include_original:\n",
    "        #    X_train = pd.concat([pipe_original.drop(label, axis=1), X_train]).reset_index(drop=True)\n",
    "        #    y_train = pd.concat([pipe_original[label], y_train]).reset_index(drop=True)\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds_proba = model.predict_proba(X_val)  # Get class probabilities\n",
    "        \n",
    "        # Store validation predictions\n",
    "        val_predictions[val_idx] = val_preds_proba\n",
    "        \n",
    "        # Evaluate model for a fold\n",
    "        val_preds = np.argmax(val_preds_proba, axis=1)  # Get predicted classes for accuracy score\n",
    "        train_score = accuracy_score(y_train, train_preds)\n",
    "        val_score = accuracy_score(y_val, val_preds)\n",
    "        \n",
    "        print(f'Fold {fold}: {val_score:.5f}')\n",
    "        \n",
    "        # Append model score for a fold to list\n",
    "        train_scores.append(train_score)\n",
    "        val_scores.append(val_score)\n",
    "    \n",
    "    # Refit the model on the entire dataset, including the original data\n",
    "    #if include_original:\n",
    "    #    X_full = pd.concat([pipe_original.drop(label, axis=1), X]).reset_index(drop=True)\n",
    "    #    y_full = pd.concat([pipe_original[label], y]).reset_index(drop=True)\n",
    "\n",
    "    X_full = column_transformers.fit_transform(X)\n",
    "    y_full = y\n",
    "\n",
    "\n",
    "    \n",
    "    model.fit(X_full, y_full)\n",
    "    \n",
    "    test_data = column_transformers.transform(test)\n",
    "\n",
    "    # Make final predictions on the test set\n",
    "    test_predictions_proba = model.predict_proba(test_data)  # Get class probabilities\n",
    "    \n",
    "    print(f'Val Score: {np.mean(val_scores):.7f} ± {np.std(val_scores):.7f} | Train Score: {np.mean(train_scores):.7f} ± {np.std(train_scores):.7f} | {target}')\n",
    "    \n",
    "    return val_scores, val_predictions, test_predictions_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_summary, oof_predictions_df, submission_predictions_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tuned = XGBClassifier(**xgb_minmax_params, random_state= 42)\n",
    "xgb2_tuned = XGBClassifier(**xgb2_minmax_params, random_state= 42)\n",
    "xgb3_tuned = XGBClassifier(**xgb3_minmax_params, random_state= 42)\n",
    "\n",
    "lgbm_tuned = LGBMClassifier(**lgbm_minmax_params, random_state= 42, n_jobs = -1)\n",
    "lgbm2_tuned = LGBMClassifier(**lgbm2_minmax_params, random_state= 42, n_jobs = -1)\n",
    "lgbm3_tuned = LGBMClassifier(**lgbm3_minmax_params, random_state= 42, n_jobs = -1)\n",
    "\n",
    "cat_tuned = CatBoostClassifier(**cat_minmax_params, random_state = 42, thread_count = -1)\n",
    "\n",
    "cv_summary['xgb'], oof_predictions_df[['xgb_0', 'xgb_1', 'xgb_2']], submission_predictions_df[['xgb_0', 'xgb_1', 'xgb_2']] = cross_validate_score(xgb_tuned, train, 5, 'Target')\n",
    "cv_summary['xgb2'], oof_predictions_df[['xgb2_0', 'xgb2_1', 'xgb2_2']], submission_predictions_df[['xgb2_0', 'xgb2_1', 'xgb2_2']] = cross_validate_score(xgb2_tuned, train, 5, 'Target')\n",
    "cv_summary['xgb3'], oof_predictions_df[['xgb3_0', 'xgb3_1', 'xgb3_2']], submission_predictions_df[['xgb3_0', 'xgb3_1', 'xgb3_2']] = cross_validate_score(xgb3_tuned, train, 5, 'Target')\n",
    "\n",
    "cv_summary['lgbm'], oof_predictions_df[['lgbm_0', 'lgbm_1', 'lgbm_2']], submission_predictions_df[['lgbm_0', 'lgbm_1', 'lgbm_2']] = cross_validate_score(lgbm_tuned, train, 5, 'Target')\n",
    "cv_summary['lgbm2'], oof_predictions_df[['lgbm2_0', 'lgbm2_1', 'lgbm2_2']], submission_predictions_df[['lgbm2_0', 'lgbm2_1', 'lgbm2_2']] = cross_validate_score(lgbm2_tuned, train, 5, 'Target')\n",
    "cv_summary['lgbm3'], oof_predictions_df[['lgbm3_0', 'lgbm3_1', 'lgbm3_2']], submission_predictions_df[['lgbm3_0', 'lgbm3_1', 'lgbm3_2']] = cross_validate_score(lgbm3_tuned, train, 5, 'Target')\n",
    "\n",
    "cv_summary['cat'], oof_predictions_df[['cat_0', 'cat_1', 'cat_2']], submission_predictions_df[['cat_0', 'cat_1', 'cat_2']] = cross_validate_score(cat_tuned, train, 5, 'Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_df = cv_summary.transpose()\n",
    "transposed_df.columns = ['fold1','fold2','fold3','fold4','fold5']\n",
    "transposed_df['Mean'] = transposed_df.mean(axis=1)\n",
    "transposed_df['Std'] = transposed_df.std(axis=1)\n",
    "transposed_df.sort_values(by = 'Mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skfold = StratifiedKFold(n_splits = 6, shuffle = True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log = True),  # Minimum loss reduction\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 10, 700),  # Number of boosting rounds\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 8),  # Maximum depth of a tree\n",
    "        'subsample': trial.suggest_float('subsample', 0.2, 1.0),  # Subsample ratio of training instances\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0),  # Subsample ratio of columns\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log = True),  # L1 regularization term\n",
    "        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0, log = True),  # Minimum sum of instance weight\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 6e-1, log = True),  # Learning rate\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log = True),  # L2 regularization term\n",
    "\n",
    "\n",
    "    }\n",
    "\n",
    "\n",
    "    xgb_regressor = XGBClassifier(**params, random_state = 42)\n",
    "    \n",
    "\n",
    "\n",
    "    score = cross_val_score(xgb_regressor, oof_predictions_df, train['Target'], scoring= 'accuracy',  cv= skfold)\n",
    "    #score = min(score.mean(), score.median())\n",
    "    score = score.mean()\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler = TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=40, n_jobs= -1)\n",
    "\n",
    "print(\"Best hyperparameters: \", study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_optimization_history(study).show()\n",
    "vis.plot_parallel_coordinate(study).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model_params = {'gamma': 3.3742991305902555e-05, 'n_estimators': 229, 'max_depth': 6, 'subsample': 0.27046522899393777, 'colsample_bytree': 0.3877794399997429, 'alpha': 2.5005198691264295e-07, 'min_child_weight': 0.01569095380228693, 'learning_rate': 0.00014463340964593574, 'lambda': 1.727921780705232e-05}\n",
    "\n",
    "#83.213\n",
    "meta_model_params2 = {'gamma': 0.5567419996563112, 'n_estimators': 198, 'max_depth': 4, 'subsample': 0.6302436248568573, 'colsample_bytree': 0.8901839913364566, 'alpha': 9.363345107468037e-06, 'min_child_weight': 0.0013704803385012033, 'learning_rate': 0.00014660365021402523, 'lambda': 1.3664437673539847e-08}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = XGBClassifier(**meta_model_params2, random_state= 42, objective = 'multi:softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "min_features_to_select = 1\n",
    "\n",
    "#Selection \n",
    "pipeline = Pipeline([\n",
    "    ('rfecv', RFECV(estimator=meta_model,\n",
    "                    step=1,\n",
    "                    cv= skfold,\n",
    "                    scoring=\"accuracy\",\n",
    "                    min_features_to_select=min_features_to_select,\n",
    "                    n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(oof_predictions_df, train['Target'])\n",
    "\n",
    "#CV score\n",
    "print(\"Best CV score: \")\n",
    "selected_features = np.array( oof_predictions_df.columns)[pipeline.named_steps['rfecv'].support_]\n",
    "print( pipeline.named_steps['rfecv'].cv_results_[\"mean_test_score\"][len(selected_features) - 1])\n",
    "\n",
    "\n",
    "\n",
    "# Selected features after RFECV\n",
    "print('Number of evaluated features:', len(oof_predictions_df.columns))\n",
    "print('Number of selected features:', len(selected_features))\n",
    "print(\"Selected Features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model.fit(oof_predictions_df, train['Target'])\n",
    "\n",
    "preds_test =  meta_model.predict(submission_predictions_df)\n",
    "submission = pd.DataFrame({'id': test.index,\n",
    "                       'Target': preds_test})\n",
    "\n",
    "submission['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = {\n",
    "    2: 'Enrolled',\n",
    "    0: 'Dropout',\n",
    "    1: 'Graduate'\n",
    "}\n",
    "# Replace the values in the \"Target\" column\n",
    "submission['Target'] = submission['Target'].replace(target_dict)\n",
    "submission.to_csv('academic-success-predictions_ensemble_minmax2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "curve = learning_curve(meta_model, oof_predictions_df, train['Target'], cv = skfold, scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(meta_model, oof_predictions_df, train['Target'], cv = skfold, scoring = 'accuracy', train_sizes=np.linspace(0.001, 0.1, 20))\n",
    "\n",
    "# Calculate the mean and standard deviation for training and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curve (SVC)\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "plt.grid()\n",
    "\n",
    "# Plot the fill between standard deviations\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "\n",
    "# Plot the mean scores\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Define your train and target data\n",
    "X = train.drop(columns=target)\n",
    "y = train[target]\n",
    "\n",
    "# Stratified K-Fold cross-validation\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initiate prediction arrays and score lists\n",
    "val_predictions = np.zeros((len(X), 3))  # Adjust for class probabilities\n",
    "train_scores, val_scores = [], []\n",
    "\n",
    "# Class weights to give more importance to class 0\n",
    "class_weights = {0: 1, 1: 1, 2: 1}\n",
    "\n",
    "# Training model and evaluating metrics\n",
    "for fold, (train_idx, val_idx) in enumerate(skfold.split(X, y)):\n",
    "    # Define train set\n",
    "    X_train, y_train = X.iloc[train_idx].reset_index(drop=True), y.iloc[train_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Define validation set\n",
    "    X_val, y_val = X.iloc[val_idx].reset_index(drop=True), y.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # Apply column transformers\n",
    "    X_train = column_transformers.fit_transform(X_train)\n",
    "    X_val = column_transformers.transform(X_val)\n",
    "\n",
    "    # Initialize the LightGBM model with class weights\n",
    "    lgbm2_tuned = LGBMClassifier(**lgbm2_minmax_params, class_weight=class_weights)\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm2_tuned.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    train_preds = lgbm2_tuned.predict(X_train)\n",
    "    val_preds_proba = lgbm2_tuned.predict_proba(X_val)  # Get class probabilities\n",
    "    val_preds = np.argmax(val_preds_proba, axis=1)\n",
    "    \n",
    "    # Store validation predictions\n",
    "    val_predictions[val_idx] = val_preds_proba\n",
    "\n",
    "# Compute metrics\n",
    "val_preds = np.argmax(val_predictions, axis=1)\n",
    "#precision, recall, _ = precision_recall_curve(y, val_preds) # Binary only\n",
    "#pr_auc = auc(recall, precision)\n",
    "precision = precision_score(y, val_preds, average=None)\n",
    "recall = recall_score(y, val_preds, average=None)\n",
    "accuracy = accuracy_score(y, val_preds)\n",
    "roc_auc = roc_auc_score(y, val_predictions, multi_class='ovr')\n",
    "f1_weighted = f1_score(y, val_preds, average='weighted')\n",
    "#f1_macro = f1_score(y, val_preds, average='macro') # Only for balanced classes\n",
    "f1_micro = f1_score(y, val_preds, average='micro')\n",
    "\n",
    "class_report = classification_report(y, val_preds)#, output_dict=True)\n",
    "conf_matrix = confusion_matrix(y, val_preds)\n",
    "\n",
    "# Display metrics\n",
    "#print(f\"Precision-Recall AUC: {pr_auc}\")\n",
    "print(f\"Precision for each class: {precision}\")\n",
    "print(f\"Recall for each class: {recall}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"Weighted F1 Score for each class: {f1_weighted}\")\n",
    "print(f\"Micro F1 Score for each class: {f1_micro}\")\n",
    "print(\"\\nClassification Report:\\n\", class_report)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "#report_df = pd.DataFrame(class_report).transpose()\n",
    "#report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ff.create_annotated_heatmap(\n",
    "    z=conf_matrix,\n",
    "    x=['Class 0', 'Class 1', 'Class 2'],\n",
    "    y=['Class 0', 'Class 1', 'Class 2'],\n",
    "    annotation_text=conf_matrix,\n",
    "    colorscale=color_map,\n",
    "    showscale=True,\n",
    "    hoverinfo=\"z\",\n",
    "    colorbar=dict(title='Count'),\n",
    "    \n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(text='Confusion Matrix', font=dict(size=20, color=DARK_TEAL)),\n",
    "    xaxis=dict(title='Predicted Label', tickangle=0, titlefont=dict(size=15, color=DARK_TEAL)),\n",
    "    yaxis=dict(title='True Label', titlefont=dict(size=15, color=DARK_TEAL)),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            x=col, y=row, text=str(conf_matrix[row][col]),\n",
    "            showarrow=False,\n",
    "            font=dict(size=12, color='black'),\n",
    "            align=\"center\"\n",
    "        ) for row in range(conf_matrix.shape[0]) for col in range(conf_matrix.shape[1])\n",
    "    ],\n",
    "    margin=dict(l=60, r=60, b=100, t=100, pad=4),\n",
    "    paper_bgcolor = BACKGROUND_COLOR,\n",
    "    plot_bgcolor = BACKGROUND_COLOR,\n",
    "    width = 1000,\n",
    "    height = 1000,\n",
    "    \n",
    "    \n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# ROC Curve for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(3):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y, val_predictions[:, i], pos_label=i)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve using Plotly\n",
    "roc_fig = go.Figure()\n",
    "\n",
    "\n",
    "for i, color in zip(range(3), (TEAL, ORANGE, DARK_TEAL)):\n",
    "    roc_fig.add_trace(go.Scatter(\n",
    "        x=fpr[i],\n",
    "        y=tpr[i],\n",
    "        mode='lines',\n",
    "        name=f'Class {i} (AUC = {roc_auc[i]:.2f})',\n",
    "        line=dict(color = color, width=2)\n",
    "    ))\n",
    "\n",
    "roc_fig.add_trace(go.Scatter(\n",
    "    x=[0, 1],\n",
    "    y=[0, 1],\n",
    "    mode='lines',\n",
    "    line=dict(color=DARK_TEAL, dash='dash'),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "roc_fig.update_layout(\n",
    "    title=dict(text='Receiver Operating Characteristic (ROC) Curve', font=dict(size=20, color=DARK_TEAL)),\n",
    "    xaxis=dict(title='False Positive Rate', titlefont=dict(size=15, color=DARK_TEAL)),\n",
    "    yaxis=dict(title='True Positive Rate', titlefont=dict(size=15, color=DARK_TEAL)),\n",
    "    legend=dict(title=dict(text='Classes', font=dict(size=15, color=DARK_TEAL))),\n",
    "    margin=dict(l=60, r=60, b=100, t=100, pad=4),\n",
    "    paper_bgcolor=BACKGROUND_COLOR,\n",
    "    plot_bgcolor=BACKGROUND_COLOR,\n",
    "    width = 1000,\n",
    "    height = 1000\n",
    ")\n",
    "roc_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Define your train and target data\n",
    "X = train.drop(columns=target)\n",
    "y = train[target]\n",
    "\n",
    "# Stratified K-Fold cross-validation\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initiate prediction arrays and score lists\n",
    "val_predictions = np.zeros((len(X), 3))  # Adjust for class probabilities\n",
    "train_scores, val_scores = [], []\n",
    "\n",
    "# Number of iterations and class weight range\n",
    "num_iterations = 10\n",
    "class_weight_range = np.linspace(1, 10, num_iterations)\n",
    "\n",
    "# Metrics storage\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "accuracy_scores = []\n",
    "roc_auc_scores = []\n",
    "f1_weighted_scores = []\n",
    "f1_micro_scores = []\n",
    "class_reports = []\n",
    "conf_matrices = []\n",
    "\n",
    "# Iterate over different class weights for class 0\n",
    "for weight in class_weight_range:\n",
    "    class_weights = {0: weight, 1: 1, 2: 1}  # Update class weights\n",
    "    \n",
    "    # Training model and evaluating metrics\n",
    "    for fold, (train_idx, val_idx) in enumerate(skfold.split(X, y)):\n",
    "        # Define train set\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        \n",
    "        # Define validation set\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Apply column transformers\n",
    "        X_train = column_transformers.fit_transform(X_train)\n",
    "        X_val = column_transformers.transform(X_val)\n",
    "\n",
    "        # Initialize the LightGBM model with class weights\n",
    "        lgbm2_tuned = LGBMClassifier(**lgbm2_minmax_params, class_weight=class_weights)\n",
    "        \n",
    "        # Train the model\n",
    "        lgbm2_tuned.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        train_preds = lgbm2_tuned.predict(X_train)\n",
    "        val_preds_proba = lgbm2_tuned.predict_proba(X_val)  # Get class probabilities\n",
    "        val_preds = np.argmax(val_preds_proba, axis=1)\n",
    "        \n",
    "        # Store validation predictions\n",
    "        val_predictions[val_idx] = val_preds_proba\n",
    "    \n",
    "    # Compute metrics for this iteration\n",
    "    val_preds = np.argmax(val_predictions, axis=1)\n",
    "    precision = precision_score(y, val_preds, average=None)\n",
    "    recall = recall_score(y, val_preds, average=None)\n",
    "    accuracy = accuracy_score(y, val_preds)\n",
    "    roc_auc = roc_auc_score(y, val_predictions, multi_class='ovr')\n",
    "    f1_weighted = f1_score(y, val_preds, average='weighted')\n",
    "    f1_micro = f1_score(y, val_preds, average='micro')\n",
    "    class_report = classification_report(y, val_preds)\n",
    "    conf_matrix = confusion_matrix(y, val_preds)\n",
    "    \n",
    "    # Append scores to lists\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    roc_auc_scores.append(roc_auc)\n",
    "    f1_weighted_scores.append(f1_weighted)\n",
    "    f1_micro_scores.append(f1_micro)\n",
    "    class_reports.append(class_report)\n",
    "    conf_matrices.append(conf_matrix)\n",
    "\n",
    "# Convert lists to arrays\n",
    "precision_scores = np.array(precision_scores)\n",
    "recall_scores = np.array(recall_scores)\n",
    "accuracy_scores = np.array(accuracy_scores)\n",
    "roc_auc_scores = np.array(roc_auc_scores)\n",
    "f1_weighted_scores = np.array(f1_weighted_scores)\n",
    "f1_micro_scores = np.array(f1_micro_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=precision_scores[:, 0], mode='lines+markers', name='Precision Class 0'))\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=precision_scores[:, 1], mode='lines+markers', name='Precision Class 1'))\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=precision_scores[:, 2], mode='lines+markers', name='Precision Class 2'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=recall_scores[:, 0], mode='lines+markers', name='Recall Class 0'))\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=recall_scores[:, 1], mode='lines+markers', name='Recall Class 1'))\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=recall_scores[:, 2], mode='lines+markers', name='Recall Class 2'))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=accuracy_scores, mode='lines+markers', name='Accuracy'))\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=roc_auc_scores, mode='lines+markers', name='ROC AUC'))\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=f1_weighted_scores, mode='lines+markers', name='Weighted F1 Score'))\n",
    "fig.add_trace(go.Scatter(x=class_weight_range, y=f1_micro_scores, mode='lines+markers', name='Micro F1 Score'))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Metrics vs. Class 0 Weight',\n",
    "    xaxis=dict(title='Class 0 Weight'),\n",
    "    yaxis=dict(title='Score'),\n",
    "    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
    "    width = 1500,\n",
    "    height = 1500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size= 0.8, shuffle = True)\n",
    "\n",
    "X_train_transformed = column_transformers.fit_transform(X_train)\n",
    "X_test_transformed = column_transformers.transform(X_test)\n",
    "\n",
    "lgbm2_tuned = LGBMClassifier(**lgbm2_minmax_params)\n",
    "        \n",
    "        # Train the model\n",
    "lgbm2_tuned.fit(X_train_transformed, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "train_preds = lgbm2_tuned.predict(X_train_transformed)\n",
    "test_preds_proba = lgbm2_tuned.predict_proba(X_test_transformed)  # Get class probabilities\n",
    "test_preds = np.argmax(test_preds_proba, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "train_score = accuracy_score(y_train, train_preds)\n",
    "print(f\"Train score: {train_score}\")\n",
    "test_score = accuracy_score(y_test, test_preds)\n",
    "print(f\"Test score: {test_score}\")\n",
    "\n",
    "pd.DataFrame(test_preds).set_index(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df_0 = pd.concat([X_test, y_test, pd.DataFrame(test_preds_proba).set_index(X_test.index)], axis = 1).rename(columns = {0:'predicted_prob_0', 1:'predicted_prob_1', 2:'predicted_prob_2'})\n",
    "predictions_df = pd.concat([predictions_df_0, pd.DataFrame(test_preds).set_index(X_test.index)], axis = 1).rename(columns = {0:'predictions'})\n",
    "predictions_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['correct_prediction'] = predictions_df.apply(lambda row: 1 if row['Target'] == row['predictions'] else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_12_FEATURES = [\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)',\n",
    "    'Tuition fees up to date',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 2nd sem (evaluations)',\n",
    "    'Curricular units 2nd sem (enrolled)',\n",
    "    'Scholarship holder',\n",
    "    'Curricular units 1st sem (evaluations)',\n",
    "    'Course',\n",
    "    'Curricular units 1st sem (enrolled)'\n",
    "]\n",
    "\n",
    "predictions_df_top_features = predictions_df[TOP_12_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_2D = TSNE(n_components=2, n_jobs=-1, random_state=42, perplexity=250)\n",
    "\n",
    "X_2D = pd.DataFrame(\n",
    "    tsne_2D.fit_transform(predictions_df_top_features), columns=[\"dim1\", \"dim2\"], index=predictions_df_top_features.index\n",
    ").join(predictions_df['correct_prediction'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_rows_axes(n_features, n_cols=5, n_rows=None):\n",
    "    n_rows = int(np.ceil(n_features / n_cols))\n",
    "    current_col = range(1, n_cols + 1)\n",
    "    current_row = range(1, n_rows + 1)\n",
    "    return n_rows, list(product(current_row, current_col))\n",
    "\n",
    "\n",
    "\n",
    "tsne_2D = TSNE(n_components=2, n_jobs=-1, random_state=42, perplexity=250)\n",
    "\n",
    "X_2D = pd.DataFrame(\n",
    "    tsne_2D.fit_transform(predictions_df_top_features), columns=[\"dim1\", \"dim2\"], index=predictions_df_top_features.index\n",
    ").join(predictions_df['correct_prediction'].astype(str))\n",
    "\n",
    "\n",
    "\n",
    "fig = px.scatter(\n",
    "    X_2D.reset_index(),\n",
    "    x=\"dim1\",\n",
    "    y=\"dim2\",\n",
    "    symbol=\"correct_prediction\",\n",
    "    symbol_sequence=[\"diamond\", \"square\"],\n",
    "    color=\"correct_prediction\",\n",
    "    color_discrete_sequence=[TEAL, ORANGE],\n",
    "    category_orders={\"Target\": (\"0\", \"1\")},\n",
    "    hover_data=\"id\",\n",
    "    opacity=0.7,\n",
    "    height=1600,\n",
    "    width=1600,\n",
    "    title=\"Test Dataset - 2D Projection with t-SNE\",\n",
    ")\n",
    "fig.update_layout(\n",
    "    font_color=FONT_COLOR,\n",
    "    title_font_size=18,\n",
    "    plot_bgcolor=BACKGROUND_COLOR,\n",
    "    paper_bgcolor=BACKGROUND_COLOR,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        xanchor=\"right\",\n",
    "        y=1.05,\n",
    "        x=1,\n",
    "        title=\"Correct or Incorrect Prediction\",\n",
    "        itemsizing=\"constant\",\n",
    "    ),\n",
    ")\n",
    "fig.update_traces(marker_size=6)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_rows_axes(n_features, n_cols=5, n_rows=None):\n",
    "    n_rows = int(np.ceil(n_features / n_cols))\n",
    "    current_col = range(1, n_cols + 1)\n",
    "    current_row = range(1, n_rows + 1)\n",
    "    return n_rows, list(product(current_row, current_col))\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Assuming predictions_df_top_features and predictions_df are defined\n",
    "\n",
    "# Initialize perplexities\n",
    "perplexities = range(10, 61, 10)  # From 10 to 60 in steps of 10\n",
    "\n",
    "# Create subplots with defined rows and columns\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=3,\n",
    "    subplot_titles=[f\"Perplexity: {perp}\" for perp in perplexities],\n",
    "    shared_xaxes=False,  # Adjust as needed\n",
    "    shared_yaxes=False,  # Adjust as needed\n",
    "    horizontal_spacing=0.06,\n",
    "    vertical_spacing=0.02,\n",
    ")\n",
    "\n",
    "# Iterate over each perplexity value and add t-SNE plots to subplots\n",
    "for i, perplexity in enumerate(perplexities, start=1):\n",
    "    tsne_2D = TSNE(n_components=2, n_jobs=-1, random_state=42, perplexity=perplexity)\n",
    "    X_2D = pd.DataFrame(tsne_2D.fit_transform(predictions_df_top_features), columns=[\"dim1\", \"dim2\"])\n",
    "\n",
    "    # Add scatter plot to subplot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_2D[\"dim1\"],\n",
    "            y=X_2D[\"dim2\"],\n",
    "            mode='markers',\n",
    "            marker=dict(color=predictions_df['correct_prediction'], colorscale='Viridis', size=8),\n",
    "            name=f\"Perplexity: {perplexity}\",\n",
    "            showlegend=True if i == 1 else False,  # Show legend only for the first plot\n",
    "        ),\n",
    "        row=(i - 1) // 3 + 1,\n",
    "        col=(i - 1) % 3 + 1,\n",
    "    )\n",
    "\n",
    "# Update layout and annotations\n",
    "fig.update_layout(\n",
    "    title=\"t-SNE Projection with Different Perplexities\",\n",
    "    title_font_size=25,\n",
    "    font_color=\"black\",  # Adjust font color as needed\n",
    "    plot_bgcolor=\"white\",  # Adjust background color as needed\n",
    "    paper_bgcolor=\"white\",  # Adjust paper color as needed\n",
    "    width=1200,\n",
    "    height=800,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        xanchor=\"right\",\n",
    "        y=1.01,\n",
    "        x=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_xaxes(title_text=\"Dimension 1\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Dimension 2\", row=1, col=1)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_n_rows_axes(n_features, n_cols=5, n_rows=None):\n",
    "    n_rows = int(np.ceil(n_features / n_cols))\n",
    "    current_col = range(1, n_cols + 1)\n",
    "    current_row = range(1, n_rows + 1)\n",
    "    return n_rows, list(product(current_row, current_col))\n",
    "\n",
    "# Initialize t-SNE with different perplexity values\n",
    "perplexities = [10, 20, 30, 40, 50, 60]\n",
    "N_COLS = 2  # Number of columns for subplots\n",
    "n_rows, axes = get_n_rows_axes(len(perplexities), n_cols=N_COLS)\n",
    "\n",
    "fig = make_subplots(rows=n_rows, cols=N_COLS, subplot_titles=[f\"Perplexity: {perp}\" for perp in perplexities])\n",
    "\n",
    "for i, perplexity in enumerate(perplexities):\n",
    "    tsne_2D = TSNE(n_components=2, n_jobs=-1, random_state=42, perplexity=perplexity)\n",
    "    X_2D = pd.DataFrame(tsne_2D.fit_transform(predictions_df_top_features), columns=[\"dim1\", \"dim2\"], index=predictions_df_top_features.index)\n",
    "\n",
    "    row_idx, col_idx = axes[i]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_2D['dim1'],\n",
    "            y=X_2D['dim2'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                symbol=predictions_df['correct_prediction'].astype(str),\n",
    "              #  symbol_sequence=[\"diamond\", \"square\"],\n",
    "                color=predictions_df['correct_prediction'],\n",
    "                #colorscale=[[0, TEAL], [1, ORANGE]],\n",
    "                opacity=0.7,\n",
    "                size=6,\n",
    "                line=dict(width=0.5, color='DarkSlateGrey')\n",
    "            ),\n",
    "            name=f\"Perplexity: {perplexity}\",\n",
    "            showlegend=True if i == 0 else False,\n",
    "           # hoverdata = 'id'\n",
    "        ),\n",
    "        row=row_idx + 1, col=col_idx + 1\n",
    "    )\n",
    "\n",
    "# Update layout and show figure\n",
    "fig.update_layout(\n",
    "    height=1600,\n",
    "    width=1600,\n",
    "    title=\"Test Dataset - 2D Projection with t-SNE\",\n",
    "    font_color=FONT_COLOR,\n",
    "    title_font_size=18,\n",
    "    plot_bgcolor=BACKGROUND_COLOR,\n",
    "    paper_bgcolor=BACKGROUND_COLOR,\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        xanchor=\"right\",\n",
    "        y=1.05,\n",
    "        x=1,\n",
    "        title=\"Correct or Incorrect Prediction\",\n",
    "        itemsizing=\"constant\",\n",
    "    ),\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Dimension 1\")\n",
    "fig.update_yaxes(title_text=\"Dimension 2\")\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-academic-success",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
